{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from scipy.stats import logistic\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_docs(path):\n",
    "    with open(path, 'r') as csv:\n",
    "        next(csv) # skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label   \n",
    "\n",
    "doc_stream = stream_docs(path='shuffled_movie_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    for _ in range(size):\n",
    "        text, label = next(doc_stream)\n",
    "        docs.append(text)\n",
    "        y.append(label)\n",
    "    return docs, y\n",
    "\n",
    "def dataAnalysis(doc):\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    wordsP = list()\n",
    "    countP = list()\n",
    "    \n",
    "    wordsN = list()\n",
    "    countN = list()\n",
    "    \n",
    "    print('\\nRead text')\n",
    "    for _ in range(100):\n",
    "        # Getting\n",
    "        x, y = get_minibatch(doc, size=500)\n",
    "        \n",
    "        for xs,ys in zip(x,y):\n",
    "            \n",
    "            # Positive\n",
    "            if ys==1:\n",
    "                for w in xs[1:-1].split():\n",
    "                    if w in wordsP:\n",
    "                        idx = wordsP.index(w)\n",
    "                        countP[idx] = countP[idx] + 1\n",
    "                    else:\n",
    "                        wordsP.append(w)\n",
    "                        countP.append(1)\n",
    "                \n",
    "            else:\n",
    "                for w in xs[1:-1].split():\n",
    "                    if w in wordsN:\n",
    "                        idx = wordsN.index(w)\n",
    "                        countN[idx] = countN[idx] + 1\n",
    "                    else:\n",
    "                        wordsN.append(w)\n",
    "                        countN.append(1)\n",
    "        \n",
    "        \n",
    "    print('\\nSorting')\n",
    "    positive = sorted(zip(countP,wordsP),reverse=True)\n",
    "    negative = sorted(zip(countN,wordsN),reverse=True)\n",
    "    \n",
    "    positive = pd.DataFrame({'Word' : [w for _,w in positive],\n",
    "                             'Count': [c for c,_ in positive]})\n",
    "    negative = pd.DataFrame({'Word' : [w for _,w in negative],\n",
    "                             'Count': [c for c,_ in negative]})\n",
    "    \n",
    "    return positive,negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive,negative = dataAnalysis(doc_stream)\n",
    "\n",
    "# Save\n",
    "positive.to_csv('positive.csv',index=False)\n",
    "negative.to_csv('negative.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select section\n",
    "n = 1000\n",
    "select_positive = positive.loc[:n,:]\n",
    "select_negative = negative.loc[:n,:]\n",
    "\n",
    "select_words_positive = select_positive['Word'].values.tolist()\n",
    "select_words_negative = select_negative['Word'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PosNeg  = list()\n",
    "coefPos = list()\n",
    "\n",
    "Pos_Neg = list()\n",
    "countPN = list()\n",
    "for ip in range( len(select_positive) ):\n",
    "    \n",
    "    w = select_positive.loc[ip,'Word']\n",
    "    \n",
    "    if w in select_words_negative:\n",
    "        count_pos = select_positive.loc[ip,'Count']\n",
    "        count_neg = select_negative.loc[select_negative['Word'] == w,'Count'].values[0]\n",
    "    \n",
    "        PosNeg.append(w)\n",
    "        coefPos.append( count_pos/count_neg )\n",
    "    else:\n",
    "        Pos_Neg.append(w)\n",
    "        countPN.append( select_positive.loc[ip,'Count'] )\n",
    "        \n",
    "interPosNeg = pd.DataFrame({'Word' : PosNeg,'Coefficient': coefPos})\n",
    "excluPosNeg = pd.DataFrame({'Word' : Pos_Neg,'Count': countPN})\n",
    "    \n",
    "interPosNeg = interPosNeg.sort_values('Coefficient',ascending = False).reset_index()\n",
    "excluPosNeg = excluPosNeg.sort_values('Count',ascending = False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NegPos  = list()\n",
    "coefNeg = list()\n",
    "\n",
    "Neg_Pos = list()\n",
    "countNP = list()\n",
    "for ip in range( len(select_negative) ):\n",
    "    \n",
    "    w = select_negative.loc[ip,'Word']\n",
    "    \n",
    "    if w in select_words_positive:\n",
    "        count_pos = select_negative.loc[ip,'Count']\n",
    "        count_neg = select_positive.loc[select_positive['Word'] == w,'Count'].values[0]\n",
    "    \n",
    "        NegPos.append(w)\n",
    "        coefNeg.append( count_pos/count_neg )\n",
    "    else:\n",
    "        Neg_Pos.append(w)\n",
    "        countNP.append(select_negative.loc[ip,'Count'])\n",
    "\n",
    "interNegPos = pd.DataFrame({'Word' : NegPos,'Coefficient': coefNeg})\n",
    "excluNegPos = pd.DataFrame({'Word' : Neg_Pos,'Count': countNP})\n",
    "\n",
    "interNegPos = interNegPos.sort_values('Coefficient',ascending = False).reset_index()\n",
    "excluNegPos = excluNegPos.sort_values('Count',ascending = False).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(texto):\n",
    "    import re\n",
    "    REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\n)\")\n",
    "    REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "    texto = REPLACE_NO_SPACE.sub('', texto.lower())\n",
    "    texto = REPLACE_WITH_SPACE.sub(' ', texto)\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positive lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positiveLexicon(text):\n",
    "    \n",
    "    # Good words list\n",
    "    with open('positive-words.txt', 'r') as f:\n",
    "        goodWords = f.read().split('\\n')[:-2]\n",
    "    \n",
    "    good = 0\n",
    "    for w in text.split():\n",
    "        if w in goodWords:\n",
    "            good = good + 1\n",
    "    \n",
    "    return good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negativeLexicon(text):\n",
    "    \n",
    "    # Good words list\n",
    "    with open('negative-words.txt', 'r') as f:\n",
    "        badWords = f.read().split('\\n')[:-2]\n",
    "    \n",
    "    bad = 0\n",
    "    for w in text.split():\n",
    "        if w in badWords:\n",
    "            bad = bad + 1\n",
    "    \n",
    "    return bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does include \"no\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doesIncludeNo(text):\n",
    "    \n",
    "    nos = ['No','no','Not','not']\n",
    "    isthereNo = 0\n",
    "    for w in text[1:-1].split():\n",
    "        if w in nos:\n",
    "            isthereNo = 1\n",
    "            break\n",
    "    \n",
    "    return isthereNo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does include Pronouns (1st and 2nd)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doesIncludePronouns(text):\n",
    "    \n",
    "    pronouns = stopwords.words('english')[17:]\n",
    "    isthere = 0\n",
    "    for w in text[1:-1].split():\n",
    "        if w in pronouns:\n",
    "            isthere = 1\n",
    "            break\n",
    "    \n",
    "    return isthere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does include \"!\"? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doesIncludeExclamationMark(text):\n",
    "    return int( '!' in text[1:-1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### log(Count words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logCountWords(text):\n",
    "    return np.log( len(text[1:-1].split()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_stream = stream_docs(path='shuffled_movie_data.csv')\n",
    "\n",
    "x = list()\n",
    "y = list()\n",
    "for _ in range(50):\n",
    "    # Getting\n",
    "    x_raw, y_raw = get_minibatch(doc_stream, size=1000)\n",
    "    \n",
    "    # Update features\n",
    "    features = [ [ positiveLexicon           (preprocessing(text)),\n",
    "                   negativeLexicon           (preprocessing(text)),\n",
    "                   doesIncludeNo             (preprocessing(text)), \n",
    "                   doesIncludePronouns       (preprocessing(text)),\n",
    "                   doesIncludeExclamationMark(preprocessing(text)),\n",
    "                   logCountWords             (preprocessing(text))] for text in x_raw ] \n",
    "    x = x + features\n",
    "    \n",
    "    # Update out\n",
    "    y = y + y_raw\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StochasticGradientDescent(x_train,y_train):\n",
    "    import random\n",
    "    from scipy.stats import logistic\n",
    "    \n",
    "    # Parameters\n",
    "    eta       = 0.001\n",
    "    err       = 1000\n",
    "    errNorm   = 1000\n",
    "    threshold = 0.00001\n",
    "    \n",
    "    n_samples  = len(x_train   )\n",
    "    n_features = len(x_train[0])\n",
    "    \n",
    "    w = np.zeros(n_features + 1)\n",
    "    \n",
    "    # Train Loop\n",
    "    while (errNorm>threshold):\n",
    "        exErr = err\n",
    "        \n",
    "        # Random selection\n",
    "        n = round(random.uniform(0, n_samples-1))\n",
    "        try:\n",
    "            xs = np.array( x_train[n] + [1] )\n",
    "        except:\n",
    "            print('n: ',n)\n",
    "            print('n_samples: ',n_samples)\n",
    "            \n",
    "        ys = y_train[n]\n",
    "        \n",
    "        # Hypotesis\n",
    "        h = logistic.cdf( np.dot(xs,w) ) \n",
    "        \n",
    "        # Gradient\n",
    "        g = (h - ys)*xs\n",
    "        \n",
    "        # Update\n",
    "        w = w - eta*g\n",
    "        \n",
    "        # Prediction\n",
    "        y_pred = w*xs\n",
    "        \n",
    "        # Error\n",
    "        err = np.sum(np.abs(ys - y_pred))\n",
    "        \n",
    "        # Update error\n",
    "        errNorm = np.abs(exErr - err)/np.abs(err)\n",
    "        \n",
    "    return w\n",
    "\n",
    "def applyModel(x,w):      \n",
    "    \n",
    "    y_pred = list()\n",
    "    for xs in x:\n",
    "        ys =  logistic.cdf( np.dot( np.array(xs + [1]),w ) ) \n",
    "        ys = int( ys > 0.5 )\n",
    "        \n",
    "        y_pred.append(ys)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=8) \n",
    "accuracy = list()\n",
    "\n",
    "for train, test in kf.split(x):    \n",
    "    # Select\n",
    "    x_train = [ x[i] for i in train ]\n",
    "    y_train = [ y[i] for i in train ]\n",
    "    \n",
    "    x_test = [ x[i] for i in test ]\n",
    "    y_test = [ y[i] for i in test ]\n",
    "    \n",
    "    # Run train\n",
    "    w = StochasticGradientDescent(x_train, y_train)\n",
    "    \n",
    "    # Run test\n",
    "    y_pred = applyModel(x_test,w)\n",
    "    \n",
    "    # Accuracy\n",
    "    acc = 0\n",
    "    for real,pred in zip(y_pred,y_test):\n",
    "        acc = acc + int( real == pred )\n",
    "    \n",
    "    accuracy.append( acc*100/len(test) )\n",
    "    \n",
    "    # Bar\n",
    "    pbar.update()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
